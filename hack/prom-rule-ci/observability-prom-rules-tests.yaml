---
rule_files:
  - /tmp/rules.verify

group_eval_order:
  - kubevirt.hyperconverged.rules

tests:
  - interval: 1m
    input_series:
      - series: 'instance:node_cpu_utilisation:rate1m{instance="n1.cnv.redhat.com"}'
        values: "0.5+0x30"
      - series: 'instance:node_cpu_utilisation:rate1m{instance="n2.cnv.redhat.com"}'
        values: "stale+0x10 0.5+0x10 0.9+0x10"

    alert_rule_test:
      - eval_time: 8m
        alertname: HighCPUWorkload
        exp_alerts: [ ]

      - eval_time: 18m
        alertname: HighCPUWorkload
        exp_alerts: [ ]

      - eval_time: 28m
        alertname: HighCPUWorkload
        exp_alerts:
          - exp_annotations:
              summary: "High CPU usage on host n2.cnv.redhat.com"
              description: "CPU utilization for n2.cnv.redhat.com has been above 90% for more than 5 minutes."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/HighCPUWorkload"
            exp_labels:
              instance: "n2.cnv.redhat.com"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"


  - interval: 1m
    input_series:
      # Control plane node - n1.cnv.redhat.com
      - series: 'kube_node_role{node="n1.cnv.redhat.com",role="control-plane"}'
        values: "1+0x30"
      # Non-control plane node - n2.cnv.redhat.com
      - series: 'kube_node_role{node="n2.cnv.redhat.com",role="worker"}'
        values: "0+0x30"
      # n1.cnv.redhat.com is ready for 10 minutes and then becomes not ready
      - series: 'kube_node_status_condition{condition="Ready",status="true",node="n1.cnv.redhat.com"}'
        values: "1+0x10 0+0x10"
      # n2.cnv.redhat.com is always not ready but doesn't have the control-plane role
      - series: 'kube_node_status_condition{condition="Ready",status="true",node="n2.cnv.redhat.com"}'
        values: "0+0x20"

    alert_rule_test:
        - eval_time: 8m
          alertname: HAControlPlaneDown
          exp_alerts: [ ]

        - eval_time: 18m
          alertname: HAControlPlaneDown
          exp_alerts:
          - exp_annotations:
              summary: "Control plane node n1.cnv.redhat.com is not ready"
              description: "Control plane node n1.cnv.redhat.com has been not ready for more than 5 minutes."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/HAControlPlaneDown"
            exp_labels:
              node: "n1.cnv.redhat.com"
              severity: "critical"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"

  - interval: 1m
    input_series:
      # Test case 1: IFF_UP is set, IFF_RUNNING and IFF_LOWER_UP are NOT set, interface is up (should alert)
      # Flag value = 1 (IFF_UP=1, IFF_RUNNING=0, IFF_LOWER_UP=0)
      - series: 'node_network_flags{instance="n1.cnv.redhat.com",device="eth0"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n1.cnv.redhat.com",device="eth0"}'
        values: "1+0x30"

      # Test case 2: IFF_UP is set and IFF_RUNNING is set (should NOT alert)
      # Flag value = 65 (IFF_UP=1, IFF_RUNNING=64, IFF_LOWER_UP=0)
      - series: 'node_network_flags{instance="n1.cnv.redhat.com",device="eth1"}'
        values: "65+0x30"
      - series: 'node_network_up{instance="n1.cnv.redhat.com",device="eth1"}'
        values: "1+0x30"

      # Test case 3: IFF_UP is NOT set and IFF_RUNNING is NOT set (should NOT alert)
      # Flag value = 0 (IFF_UP=0, IFF_RUNNING=0, IFF_LOWER_UP=0)
      - series: 'node_network_flags{instance="n1.cnv.redhat.com",device="eth2"}'
        values: "0+0x30"
      - series: 'node_network_up{instance="n1.cnv.redhat.com",device="eth2"}'
        values: "0+0x30"

      # Test case 4: IFF_UP is set, IFF_RUNNING and IFF_LOWER_UP are NOT set but device is in ignored list (should NOT alert)
      # Flag value = 1 (IFF_UP=1, IFF_RUNNING=0, IFF_LOWER_UP=0)
      - series: 'node_network_flags{instance="n1.cnv.redhat.com",device="lo"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n1.cnv.redhat.com",device="lo"}'
        values: "1+0x30"

      # Test case 5: IFF_UP is set, IFF_RUNNING and IFF_LOWER_UP are NOT set but node_network_up is 0 (should NOT alert)
      # Flag value = 1 (IFF_UP=1, IFF_RUNNING=0, IFF_LOWER_UP=0)
      - series: 'node_network_flags{instance="n1.cnv.redhat.com",device="eth3"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n1.cnv.redhat.com",device="eth3"}'
        values: "0+0x30"

      # Test case 5a: IFF_UP is set, IFF_RUNNING is NOT set but IFF_LOWER_UP is set (should NOT alert)
      # Flag value = 65537 (IFF_UP=1, IFF_RUNNING=0, IFF_LOWER_UP=65536)
      - series: 'node_network_flags{instance="n1.cnv.redhat.com",device="eth5"}'
        values: "65537+0x30"
      - series: 'node_network_up{instance="n1.cnv.redhat.com",device="eth5"}'
        values: "1+0x30"

      # Test case 6: Multiple interfaces with different states on same instance
      # Two interfaces with IFF_UP=1, IFF_RUNNING=0, IFF_LOWER_UP=0, node_network_up=1 (should alert with count=2)
      - series: 'node_network_flags{instance="n2.cnv.redhat.com",device="eth0"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n2.cnv.redhat.com",device="eth0"}'
        values: "1+0x30"
      - series: 'node_network_flags{instance="n2.cnv.redhat.com",device="eth4"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n2.cnv.redhat.com",device="eth4"}'
        values: "1+0x30"
      # One interface with IFF_UP=1, IFF_RUNNING=64 (should NOT alert)
      - series: 'node_network_flags{instance="n2.cnv.redhat.com",device="eth1"}'
        values: "65+0x30"
      - series: 'node_network_up{instance="n2.cnv.redhat.com",device="eth1"}'
        values: "1+0x30"
      # One ignored interface with IFF_UP=1, IFF_RUNNING=0 (should NOT alert)
      - series: 'node_network_flags{instance="n2.cnv.redhat.com",device="veth123"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n2.cnv.redhat.com",device="veth123"}'
        values: "1+0x30"

    alert_rule_test:
      # Test at 3m - alert should not fire yet (not enough time elapsed)
      - eval_time: 3m
        alertname: NodeNetworkInterfaceDown
        exp_alerts: [ ]

      # Test at 10m - alert should fire for both instances
      - eval_time: 10m
        alertname: NodeNetworkInterfaceDown
        exp_alerts:
          - exp_annotations:
              summary: "Network interfaces are down"
              description: "1 network devices have been down on instance n1.cnv.redhat.com for more than 5 minutes."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/NodeNetworkInterfaceDown"
            exp_labels:
              instance: "n1.cnv.redhat.com"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"
          - exp_annotations:
              summary: "Network interfaces are down"
              description: "2 network devices have been down on instance n2.cnv.redhat.com for more than 5 minutes."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/NodeNetworkInterfaceDown"
            exp_labels:
              instance: "n2.cnv.redhat.com"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"

  - interval: 1m
    input_series:
      # Test case 6: Interface transitioning between states
      # Flag values:
      # - 0 minutes to 5 minutes: 65 (IFF_UP=1, IFF_RUNNING=64, IFF_LOWER_UP=0) - healthy
      # - 5 minutes to 15 minutes: 1 (IFF_UP=1, IFF_RUNNING=0, IFF_LOWER_UP=0) - should alert after 5 minutes in this state
      # - After 15 minutes: 65 (IFF_UP=1, IFF_RUNNING=64, IFF_LOWER_UP=0) - becomes healthy again
      - series: 'node_network_flags{instance="n3.cnv.redhat.com",device="eth0"}'
        values: "65+0x5 1+0x10 65+0x15"
      - series: 'node_network_up{instance="n3.cnv.redhat.com",device="eth0"}'
        values: "1+0x30"

      # Test case 7: Interface with complex flag changes
      # Flag transitions:
      # - 0 to 8 minutes: 0 (IFF_UP=0, IFF_RUNNING=0, IFF_LOWER_UP=0) - down but shouldn't alert due to no IFF_UP
      # - 8 to 25 minutes: 1 (IFF_UP=1, IFF_RUNNING=0, IFF_LOWER_UP=0) - should alert after 5 minutes in this state
      - series: 'node_network_flags{instance="n3.cnv.redhat.com",device="eth1"}'
        values: "0+0x8 1+0x17"
      - series: 'node_network_up{instance="n3.cnv.redhat.com",device="eth1"}'
        values: "1+0x30"

      # Test case 8: Multiple ignored interfaces with same instance (should NOT alert)
      # All interfaces with problematic flags are in the ignored list
      - series: 'node_network_flags{instance="n4.cnv.redhat.com",device="lo"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n4.cnv.redhat.com",device="lo"}'
        values: "1+0x30"
      - series: 'node_network_flags{instance="n4.cnv.redhat.com",device="br-int"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n4.cnv.redhat.com",device="br-int"}'
        values: "1+0x30"
      - series: 'node_network_flags{instance="n4.cnv.redhat.com",device="veth123"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n4.cnv.redhat.com",device="veth123"}'
        values: "1+0x30"
      # Test additional ignore patterns
      - series: 'node_network_flags{instance="n4.cnv.redhat.com",device="ovs-system"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n4.cnv.redhat.com",device="ovs-system"}'
        values: "1+0x30"
      - series: 'node_network_flags{instance="n4.cnv.redhat.com",device="genev_sys_6081"}'
        values: "1+0x30"
      - series: 'node_network_up{instance="n4.cnv.redhat.com",device="genev_sys_6081"}'
        values: "1+0x30"
      # One non-ignored interface that's healthy (should NOT contribute to alert)
      - series: 'node_network_flags{instance="n4.cnv.redhat.com",device="eth0"}'
        values: "65+0x30"
      - series: 'node_network_up{instance="n4.cnv.redhat.com",device="eth0"}'
        values: "1+0x30"

    alert_rule_test:
      # At 8 minutes, no alert should fire (eth0 on n3 has only been down for 3 minutes)
      - eval_time: 8m
        alertname: NodeNetworkInterfaceDown
        exp_alerts: [ ]

      # At 12m, alert should fire for n3 with two devices down
      - eval_time: 12m
        alertname: NodeNetworkInterfaceDown
        exp_alerts:
          - exp_annotations:
              summary: "Network interfaces are down"
              description: "2 network devices have been down on instance n3.cnv.redhat.com for more than 5 minutes."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/NodeNetworkInterfaceDown"
            exp_labels:
              instance: "n3.cnv.redhat.com"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"

      # At 18m, eth1 should be alerting with one device down
      - eval_time: 18m
        alertname: NodeNetworkInterfaceDown
        exp_alerts:
          - exp_annotations:
              summary: "Network interfaces are down"
              description: "1 network devices have been down on instance n3.cnv.redhat.com for more than 5 minutes."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/NodeNetworkInterfaceDown"
            exp_labels:
              instance: "n3.cnv.redhat.com"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"

      # At 22m, eth0 has recovered but eth1 is still down
      - eval_time: 22m
        alertname: NodeNetworkInterfaceDown
        exp_alerts:
          - exp_annotations:
              summary: "Network interfaces are down"
              description: "1 network devices have been down on instance n3.cnv.redhat.com for more than 5 minutes."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/NodeNetworkInterfaceDown"
            exp_labels:
              instance: "n3.cnv.redhat.com"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"

      # No alert should fire for n4 as all down interfaces are in the ignored list
      - eval_time: 10m
        alertname: NodeNetworkInterfaceDown
        exp_alerts: [ ]

  - interval: 1m
    input_series:
      # PVC 1: Should trigger alert - less than 10% space available and will fill up in 4 days
      - series: 'kubelet_volume_stats_available_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-1",namespace="test-ns-1"}'
        values: "10000+0x5 9500+0x5 9000+0x5 8500+0x5 8000+0x30"
      - series: 'kubelet_volume_stats_capacity_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-1",namespace="test-ns-1"}'
        values: "100000+0x30"
      - series: 'kubelet_volume_stats_used_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-1",namespace="test-ns-1"}'
        values: "90000+0x5 90500+0x5 91000+0x5 91500+0x5 92000+0x30"

      # PVC 2: Should NOT trigger alert - more than 10% space available
      - series: 'kubelet_volume_stats_available_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-2",namespace="test-ns-2"}'
        values: "30000+0x30"
      - series: 'kubelet_volume_stats_capacity_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-2",namespace="test-ns-2"}'
        values: "100000+0x30"
      - series: 'kubelet_volume_stats_used_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-2",namespace="test-ns-2"}'
        values: "70000+0x30"

      # PVC 3: Would trigger alert but has ReadOnlyMany access mode - should NOT alert
      - series: 'kubelet_volume_stats_available_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-3",namespace="test-ns-3"}'
        values: "5000+0x5 4500+0x5 4000+0x5 3500+0x5 3000+0x5 2500+0x5"
      - series: 'kubelet_volume_stats_capacity_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-3",namespace="test-ns-3"}'
        values: "100000+0x30"
      - series: 'kubelet_volume_stats_used_bytes{job="kubelet",metrics_path="/metrics",persistentvolumeclaim="pvc-3",namespace="test-ns-3"}'
        values: "95000+0x5 95500+0x5 96000+0x5 96500+0x5 97000+0x5 97500+0x5"
      - series: 'kube_persistentvolumeclaim_access_mode{access_mode="ReadOnlyMany",persistentvolumeclaim="pvc-3",namespace="test-ns-3"}'
        values: "1+0x30"

    alert_rule_test:
      # At 5m, no alerts should be firing yet (not enough time elapsed)
      - eval_time: 5m
        alertname: PersistentVolumeFillingUp
        exp_alerts: [ ]

      # At 12m, PVC-1 should be alerting
      - eval_time: 12m
        alertname: PersistentVolumeFillingUp
        exp_alerts:
          - exp_annotations:
              summary: "PersistentVolume is filling up"
              description: "Based on recent sampling, the PersistentVolume claimed by pvc-1 in Namespace test-ns-1 is expected to fill up within four days. Currently 9% is available."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/PersistentVolumeFillingUp"
            exp_labels:
              persistentvolumeclaim: "pvc-1"
              namespace: "test-ns-1"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"
              job: "kubelet"
              metrics_path: "/metrics"

      # At 28m, PVC-1 should still be alerting
      - eval_time: 28m
        alertname: PersistentVolumeFillingUp
        exp_alerts:
          - exp_annotations:
              summary: "PersistentVolume is filling up"
              description: "Based on recent sampling, the PersistentVolume claimed by pvc-1 in Namespace test-ns-1 is expected to fill up within four days. Currently 8% is available."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/PersistentVolumeFillingUp"
            exp_labels:
              persistentvolumeclaim: "pvc-1"
              namespace: "test-ns-1"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"
              job: "kubelet"
              metrics_path: "/metrics"

  - interval: 1m
    input_series:
      # Test case 1: CPU frequency is above 80% of max (should alert)
      # Current frequency: 2800 MHz, Max frequency: 3000 MHz (93.3% > 80%)
      - series: 'node_cpu_frequency_hertz{instance="n1.cnv.redhat.com",cpu="0"}'
        values: "2800000000+0x30"
      - series: 'node_cpu_frequency_max_hertz{instance="n1.cnv.redhat.com",cpu="0"}'
        values: "3000000000+0x30"

      # Test case 2: CPU frequency is below 80% of max (should NOT alert)
      # Current frequency: 2000 MHz, Max frequency: 3000 MHz (66.7% < 80%)
      - series: 'node_cpu_frequency_hertz{instance="n2.cnv.redhat.com",cpu="0"}'
        values: "2000000000+0x30"
      - series: 'node_cpu_frequency_max_hertz{instance="n2.cnv.redhat.com",cpu="0"}'
        values: "3000000000+0x30"

      # Test case 3: Missing max frequency metric (should NOT alert)
      # Current frequency exists but max frequency doesn't
      - series: 'node_cpu_frequency_hertz{instance="n3.cnv.redhat.com",cpu="0"}'
        values: "2800000000+0x30"

      # Test case 4: Missing current frequency metric (should NOT alert)
      # Max frequency exists but current frequency doesn't
      - series: 'node_cpu_frequency_max_hertz{instance="n4.cnv.redhat.com",cpu="0"}'
        values: "3000000000+0x30"

    alert_rule_test:
      # At 3 minutes, no alerts should fire yet (not enough time elapsed)
      - eval_time: 3m
        alertname: HighNodeCPUFrequency
        exp_alerts: [ ]

      # At 8 minutes, alerts should fire for instances with high CPU frequency
      - eval_time: 8m
        alertname: HighNodeCPUFrequency
        exp_alerts:
          - exp_annotations:
              summary: "High CPU frequency detected on node n1.cnv.redhat.com"
              description: "CPU frequency on node n1.cnv.redhat.com (CPU 0) is 2.8GHz, which is above 80% of the maximum frequency. This may indicate high CPU utilization or thermal throttling."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/HighNodeCPUFrequency"
            exp_labels:
              instance: "n1.cnv.redhat.com"
              cpu: "0"
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"

  - interval: 1m
    input_series:
      - series: 'kubevirt_hco_memory_overcommit_percentage'
        values: "80+0x10 101+0x10"
      - series: 'kube_daemonset_metadata_generation{namespace="wasp",daemonset="wasp-agent"}'
        values: "0+0x10 1+0x10"

    alert_rule_test:
      - eval_time: 1m
        alertname: DuplicateWaspAgentDSDetected
        exp_alerts: [ ]
      - eval_time: 9m
        alertname: DuplicateWaspAgentDSDetected
        exp_alerts: [ ]
      - eval_time: 12m
        alertname: DuplicateWaspAgentDSDetected
        exp_alerts:
          - exp_annotations:
              summary: "Duplicate wasp-agent deployment detected"
              description: "Two wasp-agent deployments exist in the cluster. Please follow the instructions mentioned in the runbook to remove the duplicate deployment."
              runbook_url: "https://github.com/openshift/runbooks/blob/master/alerts/openshift-virtualization-operator/DuplicateWaspAgentDSDetected.md"
            exp_labels:
              severity: "warning"
              operator_health_impact: "none"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"

  # Test for DeprecatedMachineType alert
  - interval: 1m
    input_series:
      - series: 'kubevirt_vm_info{name="deprecated-vm", namespace="default", machine_type="pc-q35-rhel7.6.0", status_group="running"}'
        values: '1+0x10'
      - series: 'kubevirt_node_deprecated_machine_types{machine_type="pc-q35-rhel7.6.0"}'
        values: '1+0x10'

    alert_rule_test:
      - eval_time: 6m
        alertname: DeprecatedMachineType
        exp_alerts:
          - exp_annotations:
              summary: "Virtual Machine 'deprecated-vm' in namespace 'default' is using a deprecated machine type."
              description: "Virtual Machine 'deprecated-vm' in namespace 'default' is using machine type 'pc-q35-rhel7.6.0', which is deprecated. Current status: 'running'."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/DeprecatedMachineType"
            exp_labels:
              severity: "warning"
              operator_health_impact: "none"
              namespace: "default"
              name: "deprecated-vm"
              machine_type: "pc-q35-rhel7.6.0"
              kubernetes_operator_part_of: "kubevirt"
              kubernetes_operator_component: "cnv-observability"
              status_group: "running"
